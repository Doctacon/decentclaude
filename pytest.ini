[pytest]
# Pytest configuration for CLI utilities testing

# Test discovery patterns
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Test paths
testpaths = tests

# Coverage options
addopts =
    --verbose
    --cov=scripts
    --cov=bin
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml
    --cov-branch
    --cov-fail-under=80
    --strict-markers
    -ra

# Markers for different test types
markers =
    unit: Unit tests for individual functions/classes
    integration: Integration tests with external dependencies
    slow: Tests that take longer to run
    bq: Tests requiring BigQuery client
    hooks: Tests for git hooks
    worktree: Tests for worktree utilities
    benchmark: Performance benchmark tests
    bq_profile: Benchmarks for bq-profile utility
    bq_lineage: Benchmarks for bq-lineage utility
    bq_explain: Benchmarks for bq-explain utility
    bq_schema_diff: Benchmarks for bq-schema-diff utility
    ai_generate: Benchmarks for ai-generate utility
    kb_search: Benchmarks for kb search functionality

# Timeout for slow tests (seconds)
timeout = 300

# Test output options
console_output_style = progress

# pytest-benchmark configuration
[tool:pytest-benchmark]
# Disable GC during benchmarks for more consistent results
disable-gc = true

# Warm up iterations before actual benchmark
warmup = true
warmup_iterations = 3

# Minimum rounds to run for each benchmark
min_rounds = 5

# Calibration precision (seconds)
calibration_precision = 10

# Maximum time to spend on each benchmark (seconds)
max_time = 5.0

# Save benchmark data
save = true
save_data = true

# JSON output
json = false

# Compare fail thresholds (fail if regression > threshold)
compare_fail = mean:10%

# Histogram settings
histogram = true
