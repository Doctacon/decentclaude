{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Data Quality Workflow\n",
    "\n",
    "This notebook demonstrates a comprehensive data quality workflow using:\n",
    "- Table profiling\n",
    "- NULL value analysis\n",
    "- Duplicate detection\n",
    "- Data quality checks\n",
    "- Quality scorecard generation\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Add scripts to path\n",
    "sys.path.insert(0, str(Path('../scripts').resolve()))\n",
    "\n",
    "# Path to utilities\n",
    "UTILS_DIR = Path('../bin/data-utils')\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the table to analyze and quality thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table to analyze\n",
    "TABLE_ID = 'bigquery-public-data.usa_names.usa_1910_current'\n",
    "\n",
    "# Quality thresholds\n",
    "THRESHOLDS = {\n",
    "    'max_null_percentage': 10.0,      # Maximum acceptable NULL percentage\n",
    "    'min_uniqueness': 0.01,            # Minimum uniqueness ratio\n",
    "    'max_duplicate_percentage': 5.0,   # Maximum acceptable duplicate percentage\n",
    "    'min_completeness': 90.0,          # Minimum data completeness percentage\n",
    "}\n",
    "\n",
    "print(f'Analyzing table: {TABLE_ID}')\n",
    "print(f'Quality thresholds: {json.dumps(THRESHOLDS, indent=2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Profile the Table\n",
    "\n",
    "Generate a comprehensive profile of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_util(util_name, args, parse_json=True):\n",
    "    \"\"\"Run a bq-* utility and return output.\"\"\"\n",
    "    util_path = UTILS_DIR / util_name\n",
    "    cmd = [str(util_path)] + args\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f'Error: {result.stderr}')\n",
    "        return None\n",
    "    \n",
    "    if parse_json:\n",
    "        try:\n",
    "            return json.loads(result.stdout)\n",
    "        except json.JSONDecodeError:\n",
    "            print('Warning: Could not parse JSON output')\n",
    "            return result.stdout\n",
    "    return result.stdout\n",
    "\n",
    "print('Profiling table...')\n",
    "profile = run_util('bq-profile', [TABLE_ID, '--format=json'])\n",
    "\n",
    "if profile:\n",
    "    meta = profile['table_overview']\n",
    "    print(f\"\\nTable Overview:\")\n",
    "    print(f\"  Rows: {meta['num_rows']:,}\")\n",
    "    print(f\"  Size: {meta['num_bytes']:,} bytes\")\n",
    "    print(f\"  Columns: {meta['num_columns']}\")\n",
    "    print(f\"  Created: {meta['created']}\")\n",
    "    print(f\"  Modified: {meta['modified']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NULL Value Analysis\n",
    "\n",
    "Analyze NULL values across all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if profile:\n",
    "    col_stats = profile['column_statistics']\n",
    "    \n",
    "    # Extract NULL percentages\n",
    "    null_data = []\n",
    "    for col_name, stats in col_stats.items():\n",
    "        null_pct = stats.get('null_percentage', 0)\n",
    "        null_data.append({\n",
    "            'Column': col_name,\n",
    "            'NULL %': null_pct,\n",
    "            'NULL Count': stats.get('null_count', 0),\n",
    "            'Total Count': stats.get('total_count', 0),\n",
    "            'Pass': null_pct <= THRESHOLDS['max_null_percentage']\n",
    "        })\n",
    "    \n",
    "    df_nulls = pd.DataFrame(null_data).sort_values('NULL %', ascending=False)\n",
    "    \n",
    "    # Visualize NULL percentages\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['red' if not pass_check else 'green' for pass_check in df_nulls['Pass']]\n",
    "    plt.barh(df_nulls['Column'], df_nulls['NULL %'], color=colors, alpha=0.7)\n",
    "    plt.axvline(x=THRESHOLDS['max_null_percentage'], color='red', linestyle='--', \n",
    "                label=f\"Threshold: {THRESHOLDS['max_null_percentage']}%\")\n",
    "    plt.xlabel('NULL Percentage')\n",
    "    plt.title('NULL Value Analysis by Column')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nNULL Value Summary:')\n",
    "    display(df_nulls)\n",
    "    \n",
    "    # Check failures\n",
    "    failures = df_nulls[~df_nulls['Pass']]\n",
    "    if not failures.empty:\n",
    "        print(f'\\n⚠ {len(failures)} column(s) exceed NULL threshold:')\n",
    "        display(failures[['Column', 'NULL %']])\n",
    "    else:\n",
    "        print('\\n✓ All columns pass NULL value check')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Uniqueness and Duplicate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if profile:\n",
    "    # Extract uniqueness ratios\n",
    "    uniqueness_data = []\n",
    "    for col_name, stats in col_stats.items():\n",
    "        uniqueness = stats.get('uniqueness_ratio', 0)\n",
    "        uniqueness_data.append({\n",
    "            'Column': col_name,\n",
    "            'Uniqueness': uniqueness,\n",
    "            'Distinct Count': stats.get('distinct_count', 0),\n",
    "            'Total Count': stats.get('total_count', 0),\n",
    "            'Type': stats.get('data_type', 'UNKNOWN')\n",
    "        })\n",
    "    \n",
    "    df_unique = pd.DataFrame(uniqueness_data).sort_values('Uniqueness', ascending=False)\n",
    "    \n",
    "    # Visualize uniqueness\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(df_unique['Column'], df_unique['Uniqueness'], alpha=0.7, color='steelblue')\n",
    "    plt.axvline(x=1.0, color='green', linestyle='--', label='Perfect Uniqueness')\n",
    "    plt.axvline(x=THRESHOLDS['min_uniqueness'], color='orange', linestyle='--', \n",
    "                label=f\"Min Threshold: {THRESHOLDS['min_uniqueness']}\")\n",
    "    plt.xlabel('Uniqueness Ratio')\n",
    "    plt.title('Column Uniqueness Analysis')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nUniqueness Summary:')\n",
    "    display(df_unique)\n",
    "    \n",
    "    # Identify potential primary keys\n",
    "    pk_candidates = df_unique[df_unique['Uniqueness'] >= 0.99]\n",
    "    if not pk_candidates.empty:\n",
    "        print(f'\\nPotential Primary Key Candidates (uniqueness >= 0.99):')\n",
    "        display(pk_candidates[['Column', 'Uniqueness', 'Distinct Count']])\n",
    "    else:\n",
    "        print('\\n⚠ No columns with high uniqueness found (may need composite key)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Completeness Check\n",
    "\n",
    "Calculate overall data completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if profile:\n",
    "    # Calculate completeness per column\n",
    "    completeness_data = []\n",
    "    for col_name, stats in col_stats.items():\n",
    "        total = stats.get('total_count', 0)\n",
    "        non_null = stats.get('non_null_count', 0)\n",
    "        completeness = (non_null / total * 100) if total > 0 else 0\n",
    "        \n",
    "        completeness_data.append({\n",
    "            'Column': col_name,\n",
    "            'Completeness %': completeness,\n",
    "            'Non-NULL Count': non_null,\n",
    "            'Total Count': total,\n",
    "            'Pass': completeness >= THRESHOLDS['min_completeness']\n",
    "        })\n",
    "    \n",
    "    df_completeness = pd.DataFrame(completeness_data).sort_values('Completeness %')\n",
    "    \n",
    "    # Overall completeness\n",
    "    overall_completeness = df_completeness['Completeness %'].mean()\n",
    "    \n",
    "    # Visualize completeness\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['red' if not pass_check else 'green' for pass_check in df_completeness['Pass']]\n",
    "    plt.barh(df_completeness['Column'], df_completeness['Completeness %'], \n",
    "             color=colors, alpha=0.7)\n",
    "    plt.axvline(x=THRESHOLDS['min_completeness'], color='red', linestyle='--', \n",
    "                label=f\"Threshold: {THRESHOLDS['min_completeness']}%\")\n",
    "    plt.axvline(x=overall_completeness, color='blue', linestyle=':', \n",
    "                label=f\"Average: {overall_completeness:.1f}%\")\n",
    "    plt.xlabel('Completeness %')\n",
    "    plt.title('Data Completeness by Column')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nOverall Completeness: {overall_completeness:.2f}%')\n",
    "    print('\\nCompleteness Summary:')\n",
    "    display(df_completeness)\n",
    "    \n",
    "    # Check failures\n",
    "    failures = df_completeness[~df_completeness['Pass']]\n",
    "    if not failures.empty:\n",
    "        print(f'\\n⚠ {len(failures)} column(s) below completeness threshold:')\n",
    "        display(failures[['Column', 'Completeness %']])\n",
    "    else:\n",
    "        print('\\n✓ All columns meet completeness threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Type Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if profile:\n",
    "    type_dist = profile['data_type_distribution']\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = sns.color_palette('Set3', len(type_dist))\n",
    "    plt.pie(type_dist.values(), labels=type_dist.keys(), autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "    plt.title('Data Type Distribution')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nData Type Summary:')\n",
    "    df_types = pd.DataFrame([\n",
    "        {'Type': k, 'Count': v, 'Percentage': f\"{v/sum(type_dist.values())*100:.1f}%\"}\n",
    "        for k, v in sorted(type_dist.items(), key=lambda x: x[1], reverse=True)\n",
    "    ])\n",
    "    display(df_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Custom Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data quality checks\n",
    "try:\n",
    "    from data_quality import DataQualityCheck, run_all_checks\n",
    "    \n",
    "    print('Running custom data quality checks...')\n",
    "    results, all_passed = run_all_checks()\n",
    "    \n",
    "    print(f'\\nCustom Checks: {\"All Passed\" if all_passed else \"Some Failed\"}')\n",
    "    for result in results:\n",
    "        status = '✓' if result.passed else '✗'\n",
    "        print(f'{status} {result.name}: {result.message}')\n",
    "        \n",
    "except ImportError:\n",
    "    print('Data quality script not available. Skipping custom checks.')\n",
    "except Exception as e:\n",
    "    print(f'Error running custom checks: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Quality Scorecard\n",
    "\n",
    "Create a comprehensive quality scorecard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if profile:\n",
    "    # Calculate quality scores\n",
    "    scores = {\n",
    "        'NULL Values': {\n",
    "            'score': len(df_nulls[df_nulls['Pass']]) / len(df_nulls) * 100,\n",
    "            'passed': len(df_nulls[df_nulls['Pass']]),\n",
    "            'total': len(df_nulls),\n",
    "            'threshold': THRESHOLDS['max_null_percentage']\n",
    "        },\n",
    "        'Completeness': {\n",
    "            'score': overall_completeness,\n",
    "            'passed': len(df_completeness[df_completeness['Pass']]),\n",
    "            'total': len(df_completeness),\n",
    "            'threshold': THRESHOLDS['min_completeness']\n",
    "        },\n",
    "        'Uniqueness': {\n",
    "            'score': len(df_unique[df_unique['Uniqueness'] >= THRESHOLDS['min_uniqueness']]) / len(df_unique) * 100,\n",
    "            'passed': len(df_unique[df_unique['Uniqueness'] >= THRESHOLDS['min_uniqueness']]),\n",
    "            'total': len(df_unique),\n",
    "            'threshold': THRESHOLDS['min_uniqueness']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create scorecard DataFrame\n",
    "    scorecard_data = []\n",
    "    for metric, data in scores.items():\n",
    "        scorecard_data.append({\n",
    "            'Quality Metric': metric,\n",
    "            'Score': f\"{data['score']:.1f}%\",\n",
    "            'Passed': data['passed'],\n",
    "            'Total': data['total'],\n",
    "            'Status': '✓ Pass' if data['score'] >= 80 else '⚠ Warning' if data['score'] >= 60 else '✗ Fail'\n",
    "        })\n",
    "    \n",
    "    df_scorecard = pd.DataFrame(scorecard_data)\n",
    "    \n",
    "    # Overall quality score\n",
    "    overall_score = sum(s['score'] for s in scores.values()) / len(scores)\n",
    "    \n",
    "    # Display scorecard\n",
    "    print('\\n' + '='*80)\n",
    "    print(f'DATA QUALITY SCORECARD - {TABLE_ID}')\n",
    "    print(f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "    print('='*80)\n",
    "    print(f'\\nOverall Quality Score: {overall_score:.1f}%')\n",
    "    print()\n",
    "    display(df_scorecard)\n",
    "    \n",
    "    # Visualize scorecard\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Score breakdown\n",
    "    metric_names = [s['Quality Metric'] for s in scorecard_data]\n",
    "    metric_scores = [float(s['Score'].rstrip('%')) for s in scorecard_data]\n",
    "    colors_bar = ['green' if s >= 80 else 'orange' if s >= 60 else 'red' for s in metric_scores]\n",
    "    \n",
    "    axes[0].barh(metric_names, metric_scores, color=colors_bar, alpha=0.7)\n",
    "    axes[0].set_xlabel('Score %')\n",
    "    axes[0].set_title('Quality Metrics Breakdown')\n",
    "    axes[0].axvline(x=80, color='green', linestyle='--', alpha=0.5, label='Pass Threshold')\n",
    "    axes[0].axvline(x=60, color='orange', linestyle='--', alpha=0.5, label='Warning Threshold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Overall gauge\n",
    "    theta = np.linspace(0, np.pi, 100)\n",
    "    score_rad = (overall_score / 100) * np.pi\n",
    "    \n",
    "    axes[1].plot(np.cos(theta), np.sin(theta), 'k-', linewidth=2)\n",
    "    axes[1].fill_between(\n",
    "        np.cos(theta[:int(overall_score)]),\n",
    "        np.sin(theta[:int(overall_score)]),\n",
    "        0,\n",
    "        color='green' if overall_score >= 80 else 'orange' if overall_score >= 60 else 'red',\n",
    "        alpha=0.6\n",
    "    )\n",
    "    axes[1].plot([0, np.cos(score_rad)], [0, np.sin(score_rad)], 'r-', linewidth=3)\n",
    "    axes[1].text(0, -0.3, f'{overall_score:.1f}%', ha='center', fontsize=24, weight='bold')\n",
    "    axes[1].text(0, -0.5, 'Overall Quality', ha='center', fontsize=12)\n",
    "    axes[1].set_xlim(-1.2, 1.2)\n",
    "    axes[1].set_ylim(-0.7, 1.2)\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('Overall Quality Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print('\\n' + '='*80)\n",
    "    if overall_score >= 80:\n",
    "        print('✓ EXCELLENT: Data quality meets all standards')\n",
    "    elif overall_score >= 60:\n",
    "        print('⚠ WARNING: Data quality needs improvement')\n",
    "    else:\n",
    "        print('✗ CRITICAL: Data quality requires immediate attention')\n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommendations\n",
    "\n",
    "Generate actionable recommendations based on quality analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if profile:\n",
    "    recommendations = []\n",
    "    \n",
    "    # NULL value recommendations\n",
    "    high_null_cols = df_nulls[df_nulls['NULL %'] > THRESHOLDS['max_null_percentage']]\n",
    "    if not high_null_cols.empty:\n",
    "        recommendations.append({\n",
    "            'Priority': 'HIGH',\n",
    "            'Category': 'NULL Values',\n",
    "            'Issue': f\"{len(high_null_cols)} column(s) with high NULL percentages\",\n",
    "            'Action': f\"Investigate columns: {', '.join(high_null_cols['Column'].tolist())}\"\n",
    "        })\n",
    "    \n",
    "    # Completeness recommendations\n",
    "    low_completeness_cols = df_completeness[df_completeness['Completeness %'] < THRESHOLDS['min_completeness']]\n",
    "    if not low_completeness_cols.empty:\n",
    "        recommendations.append({\n",
    "            'Priority': 'HIGH',\n",
    "            'Category': 'Completeness',\n",
    "            'Issue': f\"{len(low_completeness_cols)} column(s) below completeness threshold\",\n",
    "            'Action': f\"Improve data collection for: {', '.join(low_completeness_cols['Column'].tolist())}\"\n",
    "        })\n",
    "    \n",
    "    # Primary key recommendations\n",
    "    if pk_candidates.empty:\n",
    "        recommendations.append({\n",
    "            'Priority': 'MEDIUM',\n",
    "            'Category': 'Schema',\n",
    "            'Issue': 'No unique identifier column found',\n",
    "            'Action': 'Consider adding a primary key or using a composite key'\n",
    "        })\n",
    "    \n",
    "    # Display recommendations\n",
    "    print('\\n' + '='*80)\n",
    "    print('RECOMMENDATIONS')\n",
    "    print('='*80)\n",
    "    \n",
    "    if recommendations:\n",
    "        df_recommendations = pd.DataFrame(recommendations)\n",
    "        display(df_recommendations)\n",
    "    else:\n",
    "        print('\\n✓ No critical issues found. Data quality is good!')\n",
    "    \n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete data quality workflow:\n",
    "\n",
    "1. **Table Profiling** - Comprehensive analysis of table structure and statistics\n",
    "2. **NULL Analysis** - Identification of columns with excessive NULL values\n",
    "3. **Uniqueness Analysis** - Detection of potential primary keys and duplicates\n",
    "4. **Completeness Check** - Assessment of data completeness across columns\n",
    "5. **Data Type Distribution** - Overview of column types\n",
    "6. **Custom Quality Checks** - Extensible framework for domain-specific checks\n",
    "7. **Quality Scorecard** - Comprehensive quality scoring and visualization\n",
    "8. **Recommendations** - Actionable insights for data quality improvement\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Automate this workflow with scheduled notebooks or Airflow DAGs\n",
    "- Set up alerts for quality score drops\n",
    "- Integrate with data catalog systems\n",
    "- Create quality dashboards for stakeholders\n",
    "- Implement automated remediation for common issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
