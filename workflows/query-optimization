#!/usr/bin/env bash
#
# query-optimization - Systematic BigQuery query optimization workflow
#
# This workflow uses bq-explain and bq-optimize to analyze and improve
# query performance and cost.
#
# Usage:
#   ./workflows/query-optimization <query_file> [cost_threshold_usd]
#
# Arguments:
#   query_file          Path to SQL file to optimize
#   cost_threshold_usd  Maximum acceptable cost in USD (default: 1.00)
#
# Example:
#   ./workflows/query-optimization queries/dashboard.sql 0.50
#
# Output:
#   - Original query analysis
#   - Optimized query
#   - Cost savings estimate
#   - Performance recommendations
#

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
BIN_DIR="$SCRIPT_DIR/../bin/data-utils"
QUERY_FILE="${1:-}"
COST_THRESHOLD="${2:-1.00}"

if [ -z "$QUERY_FILE" ] || [ ! -f "$QUERY_FILE" ]; then
    echo "Usage: $0 <query_file> [cost_threshold_usd]"
    echo "Example: $0 queries/dashboard.sql 0.50"
    exit 1
fi

# Create optimization directory
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OPT_ID="OPT-${TIMESTAMP}"
OPT_DIR="query-optimizations/$OPT_ID"
mkdir -p "$OPT_DIR"

ORIGINAL_QUERY="$OPT_DIR/original.sql"
OPTIMIZED_QUERY="$OPT_DIR/optimized.sql"
ANALYSIS_FILE="$OPT_DIR/analysis.txt"
REPORT_FILE="$OPT_DIR/optimization-report.md"

# Copy original query
cp "$QUERY_FILE" "$ORIGINAL_QUERY"

echo -e "${BLUE}═══════════════════════════════════════════════════════${NC}"
echo -e "${BLUE}  Query Optimization${NC}"
echo -e "${BLUE}  ID: $OPT_ID${NC}"
echo -e "${BLUE}  Query: $QUERY_FILE${NC}"
echo -e "${BLUE}  Cost Threshold: \$${COST_THRESHOLD}${NC}"
echo -e "${BLUE}═══════════════════════════════════════════════════════${NC}"
echo

# Step 1: Analyze Original Query
echo -e "${YELLOW}[Step 1/5]${NC} Analyzing Original Query"
if ! "$BIN_DIR/bq-explain" --file="$ORIGINAL_QUERY" > "$ANALYSIS_FILE"; then
    echo -e "${RED}✗ FAILED${NC}: Could not analyze query"
    exit 1
fi

# Extract cost from analysis
ORIGINAL_COST=$(grep -E "Estimated cost:|Cost:" "$ANALYSIS_FILE" | head -1 | grep -oE '\$[0-9]+\.[0-9]+' | tr -d '$' || echo "0.00")
ORIGINAL_BYTES=$(grep -E "Bytes processed:" "$ANALYSIS_FILE" | head -1 | grep -oE '[0-9.]+\s*(GB|TB|MB)' || echo "unknown")

echo "  Bytes processed: $ORIGINAL_BYTES"
echo "  Estimated cost: \$${ORIGINAL_COST}"
echo -e "${GREEN}✓ COMPLETE${NC}"
echo

# Step 2: Identify Optimization Opportunities
echo -e "${YELLOW}[Step 2/5]${NC} Identifying Optimization Opportunities"

OPPORTUNITIES=()

# Check for common issues in analysis
if grep -qi "full table scan" "$ANALYSIS_FILE"; then
    OPPORTUNITIES+=("Full table scan detected - consider adding partition/cluster filters")
fi

if grep -qi "SELECT \*" "$ORIGINAL_QUERY"; then
    OPPORTUNITIES+=("SELECT * detected - specify only needed columns")
fi

if grep -qi "no partition filter" "$ANALYSIS_FILE"; then
    OPPORTUNITIES+=("No partition filter - add WHERE clause on partition column")
fi

if grep -qi "excessive shuffle" "$ANALYSIS_FILE"; then
    OPPORTUNITIES+=("Excessive shuffle detected - review JOIN order and conditions")
fi

if [ ${#OPPORTUNITIES[@]} -eq 0 ]; then
    echo "  No obvious optimization opportunities found"
    OPPORTUNITIES+=("Query appears well-optimized - consider reviewing execution plan for further improvements")
else
    echo "  Found ${#OPPORTUNITIES[@]} optimization opportunities:"
    for opp in "${OPPORTUNITIES[@]}"; do
        echo "    - $opp"
    done
fi

echo -e "${GREEN}✓ COMPLETE${NC}"
echo

# Step 3: Generate Optimized Query
echo -e "${YELLOW}[Step 3/5]${NC} Generating Optimized Query"
if ! "$BIN_DIR/bq-optimize" --file="$ORIGINAL_QUERY" > "$OPTIMIZED_QUERY"; then
    echo -e "${YELLOW}⚠ WARNING${NC}: Could not auto-optimize query"
    echo "  Using original query (manual optimization may be needed)"
    cp "$ORIGINAL_QUERY" "$OPTIMIZED_QUERY"
fi

echo "  Optimized query saved to: $OPTIMIZED_QUERY"
echo -e "${GREEN}✓ COMPLETE${NC}"
echo

# Step 4: Analyze Optimized Query
echo -e "${YELLOW}[Step 4/5]${NC} Analyzing Optimized Query"
if ! "$BIN_DIR/bq-explain" --file="$OPTIMIZED_QUERY" >> "$ANALYSIS_FILE"; then
    echo -e "${YELLOW}⚠ WARNING${NC}: Could not analyze optimized query"
    OPTIMIZED_COST="$ORIGINAL_COST"
    OPTIMIZED_BYTES="$ORIGINAL_BYTES"
else
    # Extract optimized cost
    OPTIMIZED_COST=$(grep -E "Estimated cost:|Cost:" "$ANALYSIS_FILE" | tail -1 | grep -oE '\$[0-9]+\.[0-9]+' | tr -d '$' || echo "$ORIGINAL_COST")
    OPTIMIZED_BYTES=$(grep -E "Bytes processed:" "$ANALYSIS_FILE" | tail -1 | grep -oE '[0-9.]+\s*(GB|TB|MB)' || echo "$ORIGINAL_BYTES")
fi

# Calculate savings
COST_SAVINGS=$(echo "$ORIGINAL_COST - $OPTIMIZED_COST" | bc -l)
if (( $(echo "$ORIGINAL_COST > 0" | bc -l) )); then
    SAVINGS_PCT=$(echo "scale=1; ($COST_SAVINGS / $ORIGINAL_COST) * 100" | bc -l)
else
    SAVINGS_PCT="0.0"
fi

echo "  Optimized bytes processed: $OPTIMIZED_BYTES"
echo "  Optimized cost: \$${OPTIMIZED_COST}"
echo "  Cost savings: \$${COST_SAVINGS} (${SAVINGS_PCT}%)"
echo -e "${GREEN}✓ COMPLETE${NC}"
echo

# Step 5: Validation and Recommendations
echo -e "${YELLOW}[Step 5/5]${NC} Validation and Recommendations"

PASSED=false
if (( $(echo "$OPTIMIZED_COST <= $COST_THRESHOLD" | bc -l) )); then
    echo -e "  ${GREEN}✓ PASS${NC}: Cost \$${OPTIMIZED_COST} is within threshold \$${COST_THRESHOLD}"
    PASSED=true
else
    echo -e "  ${RED}✗ FAIL${NC}: Cost \$${OPTIMIZED_COST} exceeds threshold \$${COST_THRESHOLD}"
    echo "  Further optimization needed"
fi

echo -e "${GREEN}✓ COMPLETE${NC}"
echo

# Generate report
cat > "$REPORT_FILE" <<EOF
# Query Optimization Report: $OPT_ID

**Date**: $(date)
**Original Query**: \`$QUERY_FILE\`
**Cost Threshold**: \$${COST_THRESHOLD}
**Result**: $([ "$PASSED" = true ] && echo "✓ PASS" || echo "✗ FAIL")

## Performance Comparison

| Metric | Original | Optimized | Savings |
|--------|----------|-----------|---------|
| Bytes Processed | $ORIGINAL_BYTES | $OPTIMIZED_BYTES | - |
| Estimated Cost | \$${ORIGINAL_COST} | \$${OPTIMIZED_COST} | \$${COST_SAVINGS} (${SAVINGS_PCT}%) |

## Optimization Opportunities

EOF

for opp in "${OPPORTUNITIES[@]}"; do
    echo "- $opp" >> "$REPORT_FILE"
done

cat >> "$REPORT_FILE" <<EOF

## Original Query

\`\`\`sql
$(cat "$ORIGINAL_QUERY")
\`\`\`

## Optimized Query

\`\`\`sql
$(cat "$OPTIMIZED_QUERY")
\`\`\`

## Detailed Analysis

\`\`\`
$(cat "$ANALYSIS_FILE")
\`\`\`

## Recommendations

### Immediate Actions
1. Review the optimized query above
2. Test the optimized query with sample data
3. Compare results between original and optimized queries
4. Deploy optimized query if validation passes

### Further Optimization
- Consider materializing intermediate results as tables
- Review clustering/partitioning strategy for source tables
- Use incremental models instead of full refreshes where possible
- Cache frequently accessed results

### Monitoring
- Set up cost alerts in BigQuery
- Track query performance over time
- Monitor for query pattern changes

---

*Generated by query-optimization workflow*
EOF

echo -e "${BLUE}═══════════════════════════════════════════════════════${NC}"
echo -e "${BLUE}  Optimization Complete${NC}"
echo -e "${BLUE}  Original Cost: \$${ORIGINAL_COST}${NC}"
echo -e "${BLUE}  Optimized Cost: \$${OPTIMIZED_COST}${NC}"
echo -e "${BLUE}  Savings: \$${COST_SAVINGS} (${SAVINGS_PCT}%)${NC}"
echo -e "${BLUE}  Report: $REPORT_FILE${NC}"
echo -e "${BLUE}═══════════════════════════════════════════════════════${NC}"

if [ "$PASSED" = true ]; then
    exit 0
else
    exit 1
fi
