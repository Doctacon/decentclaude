#!/usr/bin/env python3
"""
bq-benchmark - Benchmark BigQuery query performance

Usage:
  bq-benchmark <query> [options]
  bq-benchmark --file=<sql_file> [options]

Arguments:
  query      SQL query to benchmark (use quotes for multi-line)

Options:
  --file=<path>       Read query from SQL file
  --runs=<n>          Number of benchmark runs (default: 3)
  --warmup            Include a warmup run (not counted in results)
  --no-cache          Disable query cache for benchmarking
  --baseline=<file>   Compare against baseline results from JSON file
  --save=<file>       Save benchmark results to JSON file
  --format=<format>   Output format: text, json (default: text)
  --help, -h          Show this help message

Examples:
  bq-benchmark "SELECT * FROM project.dataset.table WHERE date = '2024-01-01'"
  bq-benchmark --file=query.sql --runs=5
  bq-benchmark --file=query.sql --warmup --no-cache
  bq-benchmark --file=query.sql --save=baseline.json
  bq-benchmark --file=query.sql --baseline=baseline.json
  bq-benchmark --file=query.sql --format=json
"""

import sys
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Optional
from statistics import mean, median, stdev
from google.cloud import bigquery


class Colors:
    """ANSI color codes for terminal output"""
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    CYAN = '\033[0;36m'
    MAGENTA = '\033[0;35m'
    RESET = '\033[0m'
    BOLD = '\033[1m'
    DIM = '\033[2m'


class QueryBenchmark:
    """Benchmark query execution"""

    def __init__(self, client: bigquery.Client):
        self.client = client

    def run_benchmark(
        self,
        query: str,
        num_runs: int = 3,
        warmup: bool = False,
        use_cache: bool = True
    ) -> Dict:
        """
        Benchmark query execution.

        Args:
            query: SQL query to benchmark
            num_runs: Number of benchmark runs
            warmup: Whether to do a warmup run first
            use_cache: Whether to use query cache

        Returns:
            Dictionary with benchmark results
        """
        print(f"{Colors.CYAN}Running benchmark with {num_runs} runs...{Colors.RESET}", file=sys.stderr)

        results = {
            "query": query,
            "num_runs": num_runs,
            "warmup": warmup,
            "use_cache": use_cache,
            "runs": [],
            "statistics": {}
        }

        # Warmup run
        if warmup:
            print(f"{Colors.DIM}Warmup run...{Colors.RESET}", file=sys.stderr)
            try:
                self._execute_query(query, use_cache)
            except Exception as e:
                print(f"{Colors.RED}Warmup run failed: {str(e)}{Colors.RESET}", file=sys.stderr)
                sys.exit(1)

        # Benchmark runs
        for i in range(num_runs):
            print(f"{Colors.DIM}Run {i+1}/{num_runs}...{Colors.RESET}", file=sys.stderr)

            try:
                run_result = self._execute_query(query, use_cache)
                results['runs'].append(run_result)

                # Show progress
                elapsed = run_result['elapsed_seconds']
                print(f"{Colors.DIM}  Completed in {elapsed:.2f}s{Colors.RESET}", file=sys.stderr)

            except Exception as e:
                print(f"{Colors.RED}Run {i+1} failed: {str(e)}{Colors.RESET}", file=sys.stderr)
                sys.exit(1)

        # Calculate statistics
        results['statistics'] = self._calculate_statistics(results['runs'])

        return results

    def _execute_query(self, query: str, use_cache: bool = True) -> Dict:
        """Execute a single query and collect metrics"""

        job_config = bigquery.QueryJobConfig()
        if not use_cache:
            job_config.use_query_cache = False

        # Record start time
        start_time = time.time()

        # Execute query
        query_job = self.client.query(query, job_config=job_config)

        # Wait for completion
        query_job.result()

        # Record end time
        end_time = time.time()

        # Collect metrics
        elapsed_seconds = end_time - start_time

        result = {
            "job_id": query_job.job_id,
            "elapsed_seconds": elapsed_seconds,
            "elapsed_ms": elapsed_seconds * 1000,
            "cache_hit": query_job.cache_hit if hasattr(query_job, 'cache_hit') else False,
            "total_bytes_processed": query_job.total_bytes_processed,
            "total_bytes_billed": query_job.total_bytes_billed,
            "num_dml_affected_rows": query_job.num_dml_affected_rows if hasattr(query_job, 'num_dml_affected_rows') else None,
        }

        # Add slot time if available
        if hasattr(query_job, 'total_slot_ms'):
            result['total_slot_ms'] = query_job.total_slot_ms
            if elapsed_seconds > 0:
                result['avg_slots'] = query_job.total_slot_ms / (elapsed_seconds * 1000)

        # Add stage count
        if hasattr(query_job, 'query_plan') and query_job.query_plan:
            result['num_stages'] = len(query_job.query_plan)

        return result

    @staticmethod
    def _calculate_statistics(runs: List[Dict]) -> Dict:
        """Calculate statistics from benchmark runs"""

        elapsed_times = [r['elapsed_seconds'] for r in runs]
        bytes_processed = [r['total_bytes_processed'] for r in runs]
        cache_hits = sum(1 for r in runs if r['cache_hit'])

        stats = {
            "elapsed_seconds": {
                "min": min(elapsed_times),
                "max": max(elapsed_times),
                "mean": mean(elapsed_times),
                "median": median(elapsed_times),
            },
            "bytes_processed": {
                "min": min(bytes_processed),
                "max": max(bytes_processed),
                "mean": mean(bytes_processed),
                "median": median(bytes_processed),
            },
            "cache_hits": cache_hits,
            "cache_hit_rate": cache_hits / len(runs) if runs else 0,
        }

        # Add standard deviation if we have enough runs
        if len(elapsed_times) > 1:
            stats['elapsed_seconds']['stdev'] = stdev(elapsed_times)
            stats['elapsed_seconds']['cv'] = stdev(elapsed_times) / mean(elapsed_times) if mean(elapsed_times) > 0 else 0

        if len(bytes_processed) > 1:
            stats['bytes_processed']['stdev'] = stdev(bytes_processed)

        # Slot statistics
        if all('total_slot_ms' in r for r in runs):
            slot_ms = [r['total_slot_ms'] for r in runs]
            stats['total_slot_ms'] = {
                "min": min(slot_ms),
                "max": max(slot_ms),
                "mean": mean(slot_ms),
                "median": median(slot_ms),
            }
            if len(slot_ms) > 1:
                stats['total_slot_ms']['stdev'] = stdev(slot_ms)

        if all('avg_slots' in r for r in runs):
            avg_slots = [r['avg_slots'] for r in runs]
            stats['avg_slots'] = {
                "min": min(avg_slots),
                "max": max(avg_slots),
                "mean": mean(avg_slots),
                "median": median(avg_slots),
            }

        return stats

    @staticmethod
    def compare_with_baseline(current: Dict, baseline: Dict) -> Dict:
        """Compare current results with baseline"""

        comparison = {
            "baseline_runs": baseline['num_runs'],
            "current_runs": current['num_runs'],
            "metrics": {}
        }

        # Compare elapsed time
        baseline_time = baseline['statistics']['elapsed_seconds']['mean']
        current_time = current['statistics']['elapsed_seconds']['mean']
        time_diff = current_time - baseline_time
        time_pct = (time_diff / baseline_time * 100) if baseline_time > 0 else 0

        comparison['metrics']['elapsed_seconds'] = {
            "baseline": baseline_time,
            "current": current_time,
            "difference": time_diff,
            "percent_change": time_pct,
            "improved": time_diff < 0
        }

        # Compare bytes processed
        baseline_bytes = baseline['statistics']['bytes_processed']['mean']
        current_bytes = current['statistics']['bytes_processed']['mean']
        bytes_diff = current_bytes - baseline_bytes
        bytes_pct = (bytes_diff / baseline_bytes * 100) if baseline_bytes > 0 else 0

        comparison['metrics']['bytes_processed'] = {
            "baseline": baseline_bytes,
            "current": current_bytes,
            "difference": bytes_diff,
            "percent_change": bytes_pct,
            "improved": bytes_diff < 0
        }

        # Compare slots if available
        if 'total_slot_ms' in baseline['statistics'] and 'total_slot_ms' in current['statistics']:
            baseline_slots = baseline['statistics']['total_slot_ms']['mean']
            current_slots = current['statistics']['total_slot_ms']['mean']
            slots_diff = current_slots - baseline_slots
            slots_pct = (slots_diff / baseline_slots * 100) if baseline_slots > 0 else 0

            comparison['metrics']['total_slot_ms'] = {
                "baseline": baseline_slots,
                "current": current_slots,
                "difference": slots_diff,
                "percent_change": slots_pct,
                "improved": slots_diff < 0
            }

        return comparison


def format_bytes(bytes_value: float) -> str:
    """Format bytes into human-readable string"""
    if bytes_value is None:
        return "N/A"
    for unit in ['B', 'KB', 'MB', 'GB', 'TB', 'PB']:
        if bytes_value < 1024.0:
            return f"{bytes_value:.2f} {unit}"
        bytes_value /= 1024.0
    return f"{bytes_value:.2f} EB"


def print_text_report(result: Dict, comparison: Optional[Dict] = None):
    """Print a formatted text report of benchmark results"""

    print(f"\n{Colors.CYAN}{Colors.BOLD}BigQuery Query Benchmark Results{Colors.RESET}")
    print("=" * 80)

    # Configuration
    print(f"\n{Colors.BOLD}Configuration:{Colors.RESET}")
    print(f"  Number of Runs:  {result['num_runs']}")
    print(f"  Warmup:          {'Yes' if result['warmup'] else 'No'}")
    print(f"  Query Cache:     {'Enabled' if result['use_cache'] else 'Disabled'}")
    print(f"  Cache Hits:      {result['statistics']['cache_hits']}/{result['num_runs']} " +
          f"({result['statistics']['cache_hit_rate']*100:.0f}%)")

    # Timing statistics
    stats = result['statistics']
    print(f"\n{Colors.BOLD}Execution Time:{Colors.RESET}")
    print(f"  Min:     {stats['elapsed_seconds']['min']:.3f}s")
    print(f"  Max:     {stats['elapsed_seconds']['max']:.3f}s")
    print(f"  Mean:    {stats['elapsed_seconds']['mean']:.3f}s")
    print(f"  Median:  {stats['elapsed_seconds']['median']:.3f}s")
    if 'stdev' in stats['elapsed_seconds']:
        stdev_val = stats['elapsed_seconds']['stdev']
        cv_val = stats['elapsed_seconds']['cv']
        print(f"  Stdev:   {stdev_val:.3f}s (CV: {cv_val:.2%})")

    # Bytes processed
    print(f"\n{Colors.BOLD}Data Processed:{Colors.RESET}")
    print(f"  Min:     {format_bytes(stats['bytes_processed']['min'])}")
    print(f"  Max:     {format_bytes(stats['bytes_processed']['max'])}")
    print(f"  Mean:    {format_bytes(stats['bytes_processed']['mean'])}")
    print(f"  Median:  {format_bytes(stats['bytes_processed']['median'])}")

    # Slot statistics
    if 'total_slot_ms' in stats:
        print(f"\n{Colors.BOLD}Slot Usage:{Colors.RESET}")
        print(f"  Total Slot-ms (mean): {stats['total_slot_ms']['mean']:,.0f}")
        if 'avg_slots' in stats:
            print(f"  Avg Slots (mean):     {stats['avg_slots']['mean']:.1f}")

    # Individual runs
    print(f"\n{Colors.BOLD}Individual Runs:{Colors.RESET}")
    for i, run in enumerate(result['runs'], 1):
        cache_indicator = f"{Colors.GREEN}[CACHE]{Colors.RESET}" if run['cache_hit'] else ""
        print(f"  Run {i}: {run['elapsed_seconds']:.3f}s, " +
              f"{format_bytes(run['total_bytes_processed'])}, " +
              f"Job: {run['job_id']} {cache_indicator}")

    # Baseline comparison
    if comparison:
        print(f"\n{Colors.BOLD}Baseline Comparison:{Colors.RESET}")

        for metric_name, metric_data in comparison['metrics'].items():
            improved = metric_data['improved']
            pct = metric_data['percent_change']
            color = Colors.GREEN if improved else Colors.RED
            direction = "faster" if metric_name == 'elapsed_seconds' and improved else "slower"
            if metric_name != 'elapsed_seconds':
                direction = "less" if improved else "more"

            print(f"\n  {metric_name}:")
            print(f"    Baseline: {metric_data['baseline']}")
            print(f"    Current:  {metric_data['current']}")
            print(f"    Change:   {color}{pct:+.2f}% ({direction}){Colors.RESET}")

    # Query preview
    print(f"\n{Colors.BOLD}Query Preview:{Colors.RESET}")
    query_lines = result['query'].strip().split('\n')
    preview_lines = query_lines[:5]
    for line in preview_lines:
        print(f"  {line}")
    if len(query_lines) > 5:
        print(f"  {Colors.DIM}... ({len(query_lines) - 5} more lines){Colors.RESET}")

    print("\n" + "=" * 80)


def print_json_report(result: Dict, comparison: Optional[Dict] = None):
    """Print a JSON report of benchmark results"""
    output = {
        "num_runs": result['num_runs'],
        "warmup": result['warmup'],
        "use_cache": result['use_cache'],
        "statistics": result['statistics'],
        "runs": result['runs']
    }

    if comparison:
        output['baseline_comparison'] = comparison

    print(json.dumps(output, indent=2))


def main():
    parser = argparse.ArgumentParser(
        description="Benchmark BigQuery query performance",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    parser.add_argument("query", nargs="?", help="SQL query to benchmark")
    parser.add_argument("--file", help="Read query from SQL file")
    parser.add_argument("--runs", type=int, default=3, help="Number of benchmark runs (default: 3)")
    parser.add_argument("--warmup", action="store_true", help="Include a warmup run")
    parser.add_argument("--no-cache", action="store_true", help="Disable query cache")
    parser.add_argument("--baseline", help="Compare against baseline results from JSON file")
    parser.add_argument("--save", help="Save benchmark results to JSON file")
    parser.add_argument("--format", choices=["text", "json"], default="text",
                       help="Output format (default: text)")

    args = parser.parse_args()

    # Get query from argument or file
    if args.file:
        try:
            query = Path(args.file).read_text()
        except Exception as e:
            print(f"{Colors.RED}Error reading file {args.file}: {str(e)}{Colors.RESET}", file=sys.stderr)
            sys.exit(1)
    elif args.query:
        query = args.query
    else:
        parser.print_help()
        sys.exit(1)

    # Validate runs
    if args.runs < 1:
        print(f"{Colors.RED}Number of runs must be at least 1{Colors.RESET}", file=sys.stderr)
        sys.exit(1)

    # Initialize BigQuery client
    client = bigquery.Client()

    # Run benchmark
    benchmark = QueryBenchmark(client)
    result = benchmark.run_benchmark(
        query=query,
        num_runs=args.runs,
        warmup=args.warmup,
        use_cache=not args.no_cache
    )

    # Load baseline if provided
    comparison = None
    if args.baseline:
        try:
            baseline_data = json.loads(Path(args.baseline).read_text())
            comparison = benchmark.compare_with_baseline(result, baseline_data)
        except Exception as e:
            print(f"{Colors.YELLOW}Warning: Could not load baseline: {str(e)}{Colors.RESET}", file=sys.stderr)

    # Save results if requested
    if args.save:
        try:
            Path(args.save).write_text(json.dumps(result, indent=2))
            print(f"{Colors.GREEN}Results saved to {args.save}{Colors.RESET}", file=sys.stderr)
        except Exception as e:
            print(f"{Colors.RED}Error saving results: {str(e)}{Colors.RESET}", file=sys.stderr)

    # Print report
    if args.format == "json":
        print_json_report(result, comparison)
    else:
        print_text_report(result, comparison)


if __name__ == "__main__":
    main()
