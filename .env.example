# DecentClaude Environment Configuration Template
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# Or use the interactive wizard:
#   mayor config init

# ============================================================================
# BigQuery Configuration (Required for bq-* tools)
# ============================================================================

# Google Cloud Project ID
# Used by all BigQuery utilities for API calls
# Find: gcloud config get-value project
GOOGLE_CLOUD_PROJECT=your-project-id

# Default BigQuery dataset for queries
# Optional: Can be overridden per command
# Example: analytics, staging, production
GOOGLE_DATASET=your-default-dataset

# Path to Google Cloud service account key file
# Required for authentication to BigQuery API
# Create: https://cloud.google.com/iam/docs/creating-managing-service-account-keys
# Example: /Users/you/.config/gcloud/application_default_credentials.json
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json

# BigQuery location/region
# Optional: Defaults to US multi-region
# Options: US, EU, asia-northeast1, etc.
# GOOGLE_CLOUD_LOCATION=US

# ============================================================================
# AI Integration (Required for ai-* tools)
# ============================================================================

# Anthropic API key for Claude models
# Used by: ai-generate, ai-review, ai-docs
# Get key: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-api-key

# OpenAI API key (optional)
# Used if you want to use GPT models
# Get key: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-openai-key

# Default AI model to use
# Options: claude-sonnet-4.5, claude-opus-4.5, gpt-4, gpt-3.5-turbo
# AI_MODEL=claude-sonnet-4.5

# ============================================================================
# Observability Configuration (Optional)
# ============================================================================

# Enable metrics collection
# Set to true to track tool usage, performance, costs
METRICS_ENABLED=false

# Datadog configuration (optional)
# For sending metrics to Datadog
# DATADOG_API_KEY=your-datadog-api-key
# DATADOG_APP_KEY=your-datadog-app-key
# DATADOG_SITE=datadoghq.com

# Prometheus metrics endpoint (optional)
# Port to expose Prometheus metrics
# PROMETHEUS_PORT=9090

# Sentry error tracking (optional)
# For automatic error reporting and monitoring
# Get DSN: https://sentry.io/
# SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id
# SENTRY_ENVIRONMENT=production

# Enable structured logging
# Set to json for machine-readable logs
# Options: text, json
LOG_FORMAT=text

# Logging level
# Options: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# ============================================================================
# Cache Configuration (Optional)
# ============================================================================

# Cache directory for BigQuery metadata
# Reduces API calls by caching table schemas, partition info
# Default: ~/.cache/decentclaude/bq_metadata
CACHE_DIR=~/.cache/decentclaude

# Cache TTL in seconds
# How long to keep cached metadata before refreshing
# Default: 3600 (1 hour)
CACHE_TTL=3600

# Disable caching globally
# Override with --no-cache flag on individual commands
# CACHE_ENABLED=false

# ============================================================================
# dbt Configuration (Optional, for dbt-* tools)
# ============================================================================

# Path to dbt project directory
# Default: Current directory
# DBT_PROJECT_DIR=/path/to/dbt/project

# dbt profiles directory
# Default: ~/.dbt/
# DBT_PROFILES_DIR=~/.dbt

# dbt target (profile)
# Which profile to use from profiles.yml
# Example: dev, staging, prod
# DBT_TARGET=dev

# ============================================================================
# SQLMesh Configuration (Optional, for sqlmesh-* tools)
# ============================================================================

# SQLMesh project directory
# Default: Current directory
# SQLMESH_PROJECT_DIR=/path/to/sqlmesh/project

# SQLMesh environment
# Which environment to use
# Example: dev, staging, prod
# SQLMESH_ENV=dev

# ============================================================================
# Knowledge Base Configuration (Optional)
# ============================================================================

# Knowledge base storage directory
# Where to store tribal knowledge, solutions, patterns
KB_DIR=~/.decentclaude/kb

# Knowledge base search backend
# Options: local (filesystem), vector (embeddings)
# KB_BACKEND=local

# Vector database for semantic search (optional)
# Requires additional setup
# Options: chroma, pinecone, weaviate
# KB_VECTOR_DB=chroma
# KB_VECTOR_DB_URL=http://localhost:8000

# ============================================================================
# Workflow Configuration (Optional)
# ============================================================================

# Output directory for workflow results
# Where workflows save reports, logs, artifacts
WORKFLOW_OUTPUT_DIR=./workflow-outputs

# Default data quality threshold (percentage)
# Used by data-quality-audit workflow
DATA_QUALITY_THRESHOLD=80

# Default query cost threshold (USD)
# Used by query-optimization workflow
QUERY_COST_THRESHOLD=1.00

# ============================================================================
# Development Configuration (Optional)
# ============================================================================

# Enable debug mode
# Shows additional logging and stack traces
# DEBUG=false

# Disable color output
# Useful for CI/CD environments
# NO_COLOR=false

# Parallel execution workers
# Default number of parallel workers for batch operations
# Default: CPU count
# PARALLEL_WORKERS=4

# ============================================================================
# CI/CD Integration (Optional)
# ============================================================================

# CI environment detection
# Auto-detected: GITHUB_ACTIONS, GITLAB_CI, JENKINS_HOME, etc.
# Can be set manually for custom CI systems
# CI=true

# Fail fast in CI
# Exit immediately on first error
# CI_FAIL_FAST=true

# Output format for CI
# Use json for machine-readable output
# CI_OUTPUT_FORMAT=json

# ============================================================================
# MCP Server Configuration (Optional, for Claude Code integration)
# ============================================================================

# MCP server mode
# Enable when running as MCP server for Claude Code
# MCP_SERVER_MODE=false

# MCP server log level
# Options: DEBUG, INFO, WARNING, ERROR
# MCP_LOG_LEVEL=INFO

# ============================================================================
# Advanced Configuration (Expert users only)
# ============================================================================

# BigQuery job timeout (seconds)
# Maximum time to wait for query completion
# BIGQUERY_JOB_TIMEOUT=300

# BigQuery max results per query
# Limit result set size
# BIGQUERY_MAX_RESULTS=10000

# Retry configuration for API calls
# Number of retries and backoff
# API_MAX_RETRIES=3
# API_RETRY_BACKOFF=2

# HTTP timeout for external APIs (seconds)
# HTTP_TIMEOUT=30

# ============================================================================
# Notes
# ============================================================================
#
# Security Best Practices:
# - Never commit .env file to git (it's in .gitignore)
# - Use service accounts with minimal required permissions
# - Rotate API keys regularly
# - Use different credentials for dev/staging/prod
# - Consider using cloud secret managers in production
#
# Multi-Environment Setup:
# - Create .env.dev, .env.staging, .env.prod
# - Copy relevant template: cp .env.example .env.dev
# - Use: export ENV=dev && source .env.$ENV
#
# Validation:
# - Run: mayor config validate
# - Checks all required variables are set
# - Validates API credentials work
#
# Help:
# - See docs/guides/CONFIGURATION.md for detailed guide
# - Run: mayor config --help
